{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq with attention ",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWK7ehZEPJ5T",
        "outputId": "b4a245d9-2a92-44c4-fa32-e57d8a09d013"
      },
      "source": [
        "# ================================================================================ #\n",
        "# =========================== Goolge Colab File Upload =========================== #\n",
        "# ================================================================================ #\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import output\n",
        "# !cp 파일1 파일2 # 파일1을 파일2로 복사 붙여넣기\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/DIYA time series/AttentionLayerExample/Dataset/kor.txt\" \"kor.txt\"\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "drive  kor.txt\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "mEb21u7uP2C4",
        "outputId": "c5018308-81ae-4973-80be-c327e5c263f8"
      },
      "source": [
        "import pandas as pd\n",
        "import re \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "path = \"./kor.txt\"\n",
        "df_fra = pd.read_csv(path, names = [\"src\", \"tar\", \"lic\"], sep = \"\\t\")\n",
        "\n",
        "del df_fra[\"lic\"]\n",
        "display(df_fra)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>tar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>가.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>안녕.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>뛰어!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run.</td>\n",
              "      <td>뛰어.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who?</td>\n",
              "      <td>누구?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3718</th>\n",
              "      <td>Science fiction has undoubtedly been the inspi...</td>\n",
              "      <td>공상 과학 소설은 의심의 여지 없이 오늘날 존재하는 많은 기술에 영감을 주었어.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3719</th>\n",
              "      <td>I started a new blog. I'll do my best not to b...</td>\n",
              "      <td>난 블로그를 시작했어. 블로그를 초반에만 반짝 많이 하다가 관두는 사람처럼은 되지 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3720</th>\n",
              "      <td>I think it's a shame that some foreign languag...</td>\n",
              "      <td>몇몇 외국어 선생님이 한 번도 원어민과 공부해본 적도 없으면서 대학을 나올 수 있었...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3721</th>\n",
              "      <td>If someone who doesn't know your background sa...</td>\n",
              "      <td>만일 네 사정도 잘 모르는 사람이 원어민 같다고 말한다면 그건 그 사람이 네가 원어...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3722</th>\n",
              "      <td>Doubtless there exists in this world precisely...</td>\n",
              "      <td>의심의 여지 없이 세상에는 어떤 남자이든 정확히 딱 알맞는 여자와 결혼하거나 그 반...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3723 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    src                                                tar\n",
              "0                                                   Go.                                                 가.\n",
              "1                                                   Hi.                                                안녕.\n",
              "2                                                  Run!                                                뛰어!\n",
              "3                                                  Run.                                                뛰어.\n",
              "4                                                  Who?                                                누구?\n",
              "...                                                 ...                                                ...\n",
              "3718  Science fiction has undoubtedly been the inspi...       공상 과학 소설은 의심의 여지 없이 오늘날 존재하는 많은 기술에 영감을 주었어.\n",
              "3719  I started a new blog. I'll do my best not to b...  난 블로그를 시작했어. 블로그를 초반에만 반짝 많이 하다가 관두는 사람처럼은 되지 ...\n",
              "3720  I think it's a shame that some foreign languag...  몇몇 외국어 선생님이 한 번도 원어민과 공부해본 적도 없으면서 대학을 나올 수 있었...\n",
              "3721  If someone who doesn't know your background sa...  만일 네 사정도 잘 모르는 사람이 원어민 같다고 말한다면 그건 그 사람이 네가 원어...\n",
              "3722  Doubtless there exists in this world precisely...  의심의 여지 없이 세상에는 어떤 남자이든 정확히 딱 알맞는 여자와 결혼하거나 그 반...\n",
              "\n",
              "[3723 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSSraJoYQFti",
        "outputId": "45105e3b-2421-4b6e-a1c3-232cd278a969"
      },
      "source": [
        "# library\n",
        "import numpy as np\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.tokenize import word_tokenize\n",
        "!pip install konlpy\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "dataset = df_fra[[\"src\", \"tar\"]].iloc[:,:]\n",
        "\n",
        "# preprocessing function\n",
        "\n",
        "def preprocess_eng(sent):\n",
        "    \n",
        "    # 단어 - 구두점 사이의 공간 1 생성\n",
        "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
        "    \n",
        "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고 전부 공백 전환\n",
        "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "    \n",
        "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "    \n",
        "    return sent\n",
        "\n",
        "def preprocess_kor(sent):\n",
        "    \n",
        "    sent = word_tokenize(sent)\n",
        "    \n",
        "    return sent\n",
        "\n",
        "test_sent = \"Have you ever been to New-York?\"\n",
        "test_sent_pre = preprocess_eng(test_sent)\n",
        "\n",
        "test_sent_kor = \"너는 뉴욕에 가본 적 있니?\"\n",
        "test_sent_kor_pre = preprocess_kor(test_sent_kor)\n",
        "\n",
        "print(test_sent_pre)\n",
        "print(test_sent_kor_pre)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 297kB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/88/f817ef1af6f794e8f11313dcd1549de833f4599abcec82746ab5ed086686/JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 55.2MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Installing collected packages: beautifulsoup4, JPype1, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n",
            "Have you ever been to New York ?\n",
            "['너는', '뉴욕에', '가본', '적', '있니', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rWtofC_Q4Sm",
        "outputId": "0cea41bd-c6e2-47c6-9271-a3a9f8c97514"
      },
      "source": [
        "test_sent_kor_pre.append(\"<sos>\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "['너는', '뉴욕에', '가본', '적', '있니', '?', '<sos>', '<sos>', '<sos>', '<sos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYyvo_YEQgSu",
        "outputId": "9c0cb32b-1e61-4dfd-b292-0c327e574664"
      },
      "source": [
        "def load_data(df = dataset, data_size = None):\n",
        "    df_src = df[\"src\"]\n",
        "    df_tar = df[\"tar\"]\n",
        "    \n",
        "    if data_size is None:\n",
        "        data_size = len(df)\n",
        "        \n",
        "    encoder_input, decoder_input, decoder_output = [], [], []\n",
        "    \n",
        "    for i in range(1,data_size):\n",
        "        src_line, tar_line = df_src.iloc[i], df_tar.iloc[i]\n",
        "        \n",
        "        # source data preprocessing -> word level separation\n",
        "        src_line_input = [w for w in preprocess_eng(src_line).split()]\n",
        "        \n",
        "        # target data preprocessing -> word level separation\n",
        "        tar_line = preprocess_kor(tar_line)\n",
        "        tar_line_input = tar_line.copy()\n",
        "        tar_line_output = tar_line.copy()\n",
        "\n",
        "        tar_line_input.insert(0, \"<sos>\")\n",
        "        tar_line_output.append(\"<eos>\")\n",
        "\n",
        "        #tar_line_input = [w for w in (\"<sos>\" + tar_line).split()]\n",
        "        #tar_line_output = [w for w in (tar_line + \"<eos>\").split()]\n",
        "        \n",
        "        encoder_input.append(src_line_input)\n",
        "        decoder_input.append(tar_line_input)\n",
        "        decoder_output.append(tar_line_output)\n",
        "        \n",
        "        del src_line, tar_line, tar_line_input, tar_line_output, src_line_input\n",
        "    \n",
        "    return encoder_input, decoder_input, decoder_output\n",
        "\n",
        "\n",
        "sent_eng_in, sent_kor_in, sent_kor_out = load_data(dataset, 3000)\n",
        "\n",
        "# word set \n",
        "\n",
        "tokenizer_eng = Tokenizer(filters = \"\", lower = False)\n",
        "tokenizer_kor = Tokenizer(filters = \"\", lower = False)\n",
        "\n",
        "tokenizer_eng.fit_on_texts(sent_eng_in)\n",
        "encoder_input = tokenizer_eng.texts_to_sequences(sent_eng_in)\n",
        "\n",
        "tokenizer_kor.fit_on_texts(sent_kor_in)\n",
        "tokenizer_kor.fit_on_texts(sent_kor_out)\n",
        "\n",
        "decoder_input = tokenizer_kor.texts_to_sequences(sent_kor_in)\n",
        "decoder_output = tokenizer_kor.texts_to_sequences(sent_kor_out)\n",
        "\n",
        "encoder_input = pad_sequences(encoder_input, padding = \"post\")\n",
        "decoder_input = pad_sequences(decoder_input, padding = \"post\")\n",
        "decoder_output = pad_sequences(decoder_output, padding = \"post\")\n",
        "\n",
        "print(\"encoder_input: \",encoder_input.shape)\n",
        "print(\"decoder_input: \",decoder_input.shape)\n",
        "print(\"decoder_output: \", decoder_output.shape)\n",
        "\n",
        "# define word set for src, tar data\n",
        "\n",
        "src_vocab_size = len(tokenizer_eng.word_index) + 1\n",
        "tar_vocab_size = len(tokenizer_kor.word_index) + 1\n",
        "print(\"영어 단어 집합의 크기 : {:d}, 한국어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))\n",
        "\n",
        "src_to_index = tokenizer_eng.word_index\n",
        "index_to_src = tokenizer_eng.index_word # 훈련 후 결과 비교할 때 사용\n",
        "\n",
        "tar_to_index = tokenizer_kor.word_index # 훈련 후 예측 과정에서 사용\n",
        "index_to_tar = tokenizer_kor.index_word # 훈련 후 결과 비교할 때 사용\n",
        "\n",
        "# train test split\n",
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "n_of_val = int(len(indices) * 0.1)\n",
        "\n",
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_output_train = decoder_output[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_output_test = decoder_output[-n_of_val:]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder_input:  (2999, 11)\n",
            "decoder_input:  (2999, 11)\n",
            "decoder_output:  (2999, 11)\n",
            "영어 단어 집합의 크기 : 2221, 한국어 단어 집합의 크기 : 4195\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj3f8xn_Ql0_"
      },
      "source": [
        "# model architecture\n",
        "import tensorflow as tf\n",
        "\n",
        "# seq2seq model(train)\n",
        "def seq2seq_model(\n",
        "    encoder_input_shapes = encoder_input_train.shape[1:],\n",
        "    decoder_input_shapes = decoder_input_train.shape[1:],\n",
        "    embed_dim = 64):\n",
        "\n",
        "    # for test model: translation 과정에서 재호출할 필요가 있는 레이어 집합\n",
        "    layers_test = []\n",
        "\n",
        "    # encoder\n",
        "    encoder_inputs = tf.keras.layers.Input(shape = encoder_input_shapes, name = \"encoder_input\")\n",
        "    x_enc = tf.keras.layers.Embedding(src_vocab_size, embed_dim, name = \"encoder_embed\")(encoder_inputs)\n",
        "    x_enc = tf.keras.layers.Masking(mask_value = 0.0)(x_enc)\n",
        "    x_enc, h, c = tf.keras.layers.LSTM(256, return_sequences = True, return_state = True, name = \"encoder_lstm\")(x_enc)\n",
        "    enc_states = [h,c]\n",
        "\n",
        "    layers_test.append(encoder_inputs)\n",
        "    layers_test.append(enc_states)\n",
        "    \n",
        "    # decoder\n",
        "    decoder_inputs = tf.keras.layers.Input(shape = decoder_input_shapes, name = \"decoder_input\")\n",
        "    decoder_embed = tf.keras.layers.Embedding(tar_vocab_size, embed_dim, name = \"decoder_embed\")\n",
        "    x_dec = decoder_embed(decoder_inputs)\n",
        "    x_dec = tf.keras.layers.Masking(mask_value = 0.0)(x_dec)\n",
        "    \n",
        "    decoder_lstm = tf.keras.layers.LSTM(256, return_sequences = True, return_state = True, name = \"decoder_lstm\")\n",
        "    dec_outputs, _, _ = decoder_lstm(x_dec, initial_state = enc_states)\n",
        "\n",
        "    decoder_softmax = tf.keras.layers.Dense(tar_vocab_size, activation = \"softmax\")\n",
        "    decoder_outputs = decoder_softmax(dec_outputs)\n",
        "    \n",
        "    model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs, name = \"seq2seq\")\n",
        "    model.summary()\n",
        "\n",
        "    layers_test.append(decoder_inputs)\n",
        "    layers_test.append(decoder_embed)\n",
        "    layers_test.append(decoder_lstm)\n",
        "    layers_test.append(decoder_softmax)\n",
        "    \n",
        "    # compile model\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "    loss = 'sparse_categorical_crossentropy'\n",
        "    metrics = [\"acc\"]\n",
        "    model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n",
        "\n",
        "    return model, layers_test\n",
        "\n",
        "# seq2seq model(test)\n",
        "def test_seq2seq_model(train_model, layers_test, hidden_dims = 256):\n",
        "\n",
        "    # reload the trained layer\n",
        "    encoder_inputs = layers_test[0]\n",
        "    encoder_states = layers_test[1]\n",
        "    decoder_inputs = layers_test[2]\n",
        "    decoder_embed = layers_test[3]\n",
        "    decoder_lstm = layers_test[4]\n",
        "    decoder_softmax = layers_test[5]\n",
        "\n",
        "    # encoder model\n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states, name = \"encoder\")\n",
        "\n",
        "    # decoder model\n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape = (hidden_dims,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape = (hidden_dims,))\n",
        "    decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "    dec_embed2 = decoder_embed(decoder_inputs)\n",
        "\n",
        "    decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(dec_embed2, initial_state = decoder_state_inputs)\n",
        "\n",
        "    decoder_states = [decoder_state_h, decoder_state_c]\n",
        "\n",
        "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
        "\n",
        "    decoder_model = tf.keras.models.Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states, name = \"decoder\")\n",
        "\n",
        "    return encoder_model, decoder_model\n",
        "\n",
        "\n",
        "# prediction(translate)\n",
        "\n",
        "def decode_sequence(input_seq, encoder_model, decoder_model):\n",
        "    state_h = encoder_model.predict(input_seq)\n",
        "\n",
        "    #<sos> 에 대응되는 정수 인덱스 부여\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0,0] = tar_to_index['<sos>']\n",
        "    \n",
        "    stop_cond = False\n",
        "    decode_sentence = ''\n",
        "\n",
        "    while not stop_cond:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + state_h)\n",
        "\n",
        "        # 예측 결과를 단어로 변환\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        if sampled_token_index ==0:\n",
        "            break\n",
        "        else:\n",
        "            sampled_char = index_to_tar[sampled_token_index]\n",
        "\n",
        "        # 결과를 문장에 추가\n",
        "        decode_sentence += \" \" + sampled_char \n",
        "\n",
        "        # stop condition\n",
        "        if (sampled_char == '<eos>' or len(decode_sentence) >= 50):\n",
        "            stop_cond = True\n",
        "\n",
        "        # 예측된 결과를 다음 hidden_state로 반영\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0,0] = sampled_token_index\n",
        "\n",
        "        state_h = [h,c]\n",
        "    \n",
        "    return decode_sentence"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1YLew3nd7js",
        "outputId": "b324d900-1e3b-4d92-e400-d32043c3c4bc"
      },
      "source": [
        "train_model, layers_test = seq2seq_model()\n",
        "\n",
        "# train model\n",
        "train_model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_output_train, \n",
        "          validation_data = ([encoder_input_test, decoder_input_test], decoder_output_test), epochs = 64,\n",
        "         batch_size = 64, verbose = 1)\n",
        "\n",
        "enc, dec = test_seq2seq_model(train_model, layers_test, hidden_dims = 256)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"seq2seq\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      [(None, 11)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decoder_input (InputLayer)      [(None, 11)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_embed (Embedding)       (None, 11, 64)       142144      encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "decoder_embed (Embedding)       (None, 11, 64)       268480      decoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "masking (Masking)               (None, 11, 64)       0           encoder_embed[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "masking_1 (Masking)             (None, 11, 64)       0           decoder_embed[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "encoder_lstm (LSTM)             [(None, 11, 256), (N 328704      masking[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder_lstm (LSTM)             [(None, 11, 256), (N 328704      masking_1[0][0]                  \n",
            "                                                                 encoder_lstm[0][1]               \n",
            "                                                                 encoder_lstm[0][2]               \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 11, 4195)     1078115     decoder_lstm[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 2,146,147\n",
            "Trainable params: 2,146,147\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/64\n",
            "43/43 [==============================] - 12s 68ms/step - loss: 4.7824 - acc: 0.4877 - val_loss: 4.4541 - val_acc: 0.3676\n",
            "Epoch 2/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 2.9536 - acc: 0.5428 - val_loss: 4.3815 - val_acc: 0.3977\n",
            "Epoch 3/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 2.7850 - acc: 0.5714 - val_loss: 4.2437 - val_acc: 0.4190\n",
            "Epoch 4/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 2.5807 - acc: 0.6479 - val_loss: 4.1753 - val_acc: 0.4922\n",
            "Epoch 5/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 2.4582 - acc: 0.6681 - val_loss: 4.0571 - val_acc: 0.5293\n",
            "Epoch 6/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 2.3824 - acc: 0.6682 - val_loss: 4.1012 - val_acc: 0.5290\n",
            "Epoch 7/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 2.3266 - acc: 0.6687 - val_loss: 4.1025 - val_acc: 0.5299\n",
            "Epoch 8/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 2.2800 - acc: 0.6704 - val_loss: 4.1129 - val_acc: 0.5336\n",
            "Epoch 9/64\n",
            "43/43 [==============================] - 1s 19ms/step - loss: 2.2383 - acc: 0.6714 - val_loss: 4.1351 - val_acc: 0.5330\n",
            "Epoch 10/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 2.1996 - acc: 0.6719 - val_loss: 4.1782 - val_acc: 0.5348\n",
            "Epoch 11/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 2.1633 - acc: 0.6741 - val_loss: 4.1705 - val_acc: 0.5339\n",
            "Epoch 12/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 2.1289 - acc: 0.6753 - val_loss: 4.1846 - val_acc: 0.5333\n",
            "Epoch 13/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 2.0977 - acc: 0.6772 - val_loss: 4.2356 - val_acc: 0.5296\n",
            "Epoch 14/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 2.0641 - acc: 0.6801 - val_loss: 4.2500 - val_acc: 0.5299\n",
            "Epoch 15/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 2.0296 - acc: 0.6840 - val_loss: 4.3061 - val_acc: 0.5293\n",
            "Epoch 16/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 1.9945 - acc: 0.6864 - val_loss: 4.3329 - val_acc: 0.5227\n",
            "Epoch 17/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 1.9617 - acc: 0.6869 - val_loss: 4.3079 - val_acc: 0.5263\n",
            "Epoch 18/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 1.9278 - acc: 0.6892 - val_loss: 4.3895 - val_acc: 0.5233\n",
            "Epoch 19/64\n",
            "43/43 [==============================] - 1s 19ms/step - loss: 1.8926 - acc: 0.6913 - val_loss: 4.3728 - val_acc: 0.5333\n",
            "Epoch 20/64\n",
            "43/43 [==============================] - 1s 19ms/step - loss: 1.8538 - acc: 0.6934 - val_loss: 4.4304 - val_acc: 0.5324\n",
            "Epoch 21/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 1.8139 - acc: 0.6982 - val_loss: 4.4183 - val_acc: 0.5360\n",
            "Epoch 22/64\n",
            "43/43 [==============================] - 1s 19ms/step - loss: 1.7700 - acc: 0.7024 - val_loss: 4.4368 - val_acc: 0.5369\n",
            "Epoch 23/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 1.7252 - acc: 0.7081 - val_loss: 4.4676 - val_acc: 0.5430\n",
            "Epoch 24/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 1.6789 - acc: 0.7123 - val_loss: 4.4828 - val_acc: 0.5445\n",
            "Epoch 25/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 1.6348 - acc: 0.7152 - val_loss: 4.5194 - val_acc: 0.5458\n",
            "Epoch 26/64\n",
            "43/43 [==============================] - 1s 19ms/step - loss: 1.5909 - acc: 0.7201 - val_loss: 4.5320 - val_acc: 0.5452\n",
            "Epoch 27/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 1.5466 - acc: 0.7225 - val_loss: 4.5596 - val_acc: 0.5503\n",
            "Epoch 28/64\n",
            "43/43 [==============================] - 1s 21ms/step - loss: 1.5032 - acc: 0.7267 - val_loss: 4.5861 - val_acc: 0.5497\n",
            "Epoch 29/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 1.4621 - acc: 0.7293 - val_loss: 4.6222 - val_acc: 0.5485\n",
            "Epoch 30/64\n",
            "43/43 [==============================] - 1s 19ms/step - loss: 1.4284 - acc: 0.7315 - val_loss: 4.6353 - val_acc: 0.5537\n",
            "Epoch 31/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 1.3810 - acc: 0.7368 - val_loss: 4.6709 - val_acc: 0.5515\n",
            "Epoch 32/64\n",
            "43/43 [==============================] - 1s 19ms/step - loss: 1.3398 - acc: 0.7419 - val_loss: 4.7071 - val_acc: 0.5512\n",
            "Epoch 33/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 1.3009 - acc: 0.7457 - val_loss: 4.7426 - val_acc: 0.5476\n",
            "Epoch 34/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 1.2689 - acc: 0.7496 - val_loss: 4.7433 - val_acc: 0.5506\n",
            "Epoch 35/64\n",
            "43/43 [==============================] - 1s 19ms/step - loss: 1.2297 - acc: 0.7556 - val_loss: 4.7839 - val_acc: 0.5537\n",
            "Epoch 36/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 1.1929 - acc: 0.7610 - val_loss: 4.7885 - val_acc: 0.5576\n",
            "Epoch 37/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 1.1568 - acc: 0.7689 - val_loss: 4.8311 - val_acc: 0.5537\n",
            "Epoch 38/64\n",
            "43/43 [==============================] - 1s 19ms/step - loss: 1.1225 - acc: 0.7748 - val_loss: 4.8528 - val_acc: 0.5485\n",
            "Epoch 39/64\n",
            "43/43 [==============================] - 1s 19ms/step - loss: 1.0887 - acc: 0.7806 - val_loss: 4.8884 - val_acc: 0.5506\n",
            "Epoch 40/64\n",
            "43/43 [==============================] - 1s 19ms/step - loss: 1.0574 - acc: 0.7882 - val_loss: 4.8993 - val_acc: 0.5488\n",
            "Epoch 41/64\n",
            "43/43 [==============================] - 1s 19ms/step - loss: 1.0207 - acc: 0.7972 - val_loss: 4.9231 - val_acc: 0.5528\n",
            "Epoch 42/64\n",
            "43/43 [==============================] - 1s 19ms/step - loss: 0.9852 - acc: 0.8028 - val_loss: 4.9644 - val_acc: 0.5482\n",
            "Epoch 43/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 0.9521 - acc: 0.8107 - val_loss: 4.9615 - val_acc: 0.5524\n",
            "Epoch 44/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 0.9254 - acc: 0.8164 - val_loss: 4.9861 - val_acc: 0.5579\n",
            "Epoch 45/64\n",
            "43/43 [==============================] - 1s 19ms/step - loss: 0.8998 - acc: 0.8207 - val_loss: 5.0087 - val_acc: 0.5485\n",
            "Epoch 46/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 0.8691 - acc: 0.8267 - val_loss: 5.0405 - val_acc: 0.5509\n",
            "Epoch 47/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 0.8394 - acc: 0.8327 - val_loss: 5.0589 - val_acc: 0.5570\n",
            "Epoch 48/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 0.8146 - acc: 0.8375 - val_loss: 5.0863 - val_acc: 0.5521\n",
            "Epoch 49/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 0.7924 - acc: 0.8426 - val_loss: 5.0994 - val_acc: 0.5534\n",
            "Epoch 50/64\n",
            "43/43 [==============================] - 1s 19ms/step - loss: 0.7688 - acc: 0.8465 - val_loss: 5.1409 - val_acc: 0.5549\n",
            "Epoch 51/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 0.7451 - acc: 0.8507 - val_loss: 5.1845 - val_acc: 0.5491\n",
            "Epoch 52/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 0.7220 - acc: 0.8556 - val_loss: 5.1804 - val_acc: 0.5546\n",
            "Epoch 53/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 0.6988 - acc: 0.8598 - val_loss: 5.2107 - val_acc: 0.5558\n",
            "Epoch 54/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 0.6762 - acc: 0.8627 - val_loss: 5.2236 - val_acc: 0.5540\n",
            "Epoch 55/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 0.6584 - acc: 0.8653 - val_loss: 5.2360 - val_acc: 0.5528\n",
            "Epoch 56/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 0.6405 - acc: 0.8700 - val_loss: 5.2627 - val_acc: 0.5555\n",
            "Epoch 57/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 0.6248 - acc: 0.8710 - val_loss: 5.2683 - val_acc: 0.5491\n",
            "Epoch 58/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 0.6086 - acc: 0.8734 - val_loss: 5.2954 - val_acc: 0.5491\n",
            "Epoch 59/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 0.5922 - acc: 0.8774 - val_loss: 5.3087 - val_acc: 0.5509\n",
            "Epoch 60/64\n",
            "43/43 [==============================] - 1s 19ms/step - loss: 0.5710 - acc: 0.8800 - val_loss: 5.3409 - val_acc: 0.5512\n",
            "Epoch 61/64\n",
            "43/43 [==============================] - 1s 19ms/step - loss: 0.5549 - acc: 0.8849 - val_loss: 5.3483 - val_acc: 0.5518\n",
            "Epoch 62/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 0.5443 - acc: 0.8844 - val_loss: 5.3708 - val_acc: 0.5512\n",
            "Epoch 63/64\n",
            "43/43 [==============================] - 1s 19ms/step - loss: 0.5311 - acc: 0.8875 - val_loss: 5.3704 - val_acc: 0.5528\n",
            "Epoch 64/64\n",
            "43/43 [==============================] - 1s 20ms/step - loss: 0.5176 - acc: 0.8906 - val_loss: 5.4094 - val_acc: 0.5518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q_n4N39RiOy"
      },
      "source": [
        "# test\n",
        "input_sentence = \"I love chicken very much~!\"\n",
        "decode_sentence(input_seq, enc, dec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JALfGvjsfUu2",
        "outputId": "72502aa3-4920-4c99-d757-311270636314"
      },
      "source": [
        "def seq2src(input_seq):\n",
        "    temp=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            temp = temp + index_to_src[i]+' '\n",
        "    return temp\n",
        "\n",
        "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2tar(input_seq):\n",
        "    temp=''\n",
        "    for i in input_seq:\n",
        "        if((i!= 0 and i!=tar_to_index['<sos>']) and i!=tar_to_index['<eos>']):\n",
        "            temp = temp + index_to_tar[i] + ' '\n",
        "    return temp\n",
        "\n",
        "for seq_index in [3,10,25,50,100,150,300,500,750,1000,1500, 1750, 2000, 2500]:\n",
        "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq, enc, dec)\n",
        "\n",
        "  print(\"원문 : \",seq2src(encoder_input_train[seq_index]))\n",
        "  print(\"번역문 :\",seq2tar(decoder_input_train[seq_index]))\n",
        "  print(\"예측문 :\",decoded_sentence)\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 11) for input KerasTensor(type_spec=TensorSpec(shape=(None, 11), dtype=tf.float32, name='decoder_input'), name='decoder_input', description=\"created by layer 'decoder_input'\"), but it was called on an input with incompatible shape (None, 1).\n",
            "원문 :  Who ? \n",
            "번역문 : 누구 ? \n",
            "예측문 :  누구 ? <eos>\n",
            "\n",
            "\n",
            "원문 :  Wait ! \n",
            "번역문 : 잠깐 ! \n",
            "예측문 :  이렇게나 아름다울 수가 ! <eos>\n",
            "\n",
            "\n",
            "원문 :  Got it ! \n",
            "번역문 : 알겠어 ! \n",
            "예측문 :  조심해 ! <eos>\n",
            "\n",
            "\n",
            "원문 :  Hit Tom . \n",
            "번역문 : 톰을 때려 . \n",
            "예측문 :  톰을 때려 . <eos>\n",
            "\n",
            "\n",
            "원문 :  Tom paid . \n",
            "번역문 : 톰이 지불했어 . \n",
            "예측문 :  톰이 입대했어 . <eos>\n",
            "\n",
            "\n",
            "원문 :  Of course . \n",
            "번역문 : 물론이죠 . \n",
            "예측문 :  일어나 ! <eos>\n",
            "\n",
            "\n",
            "원문 :  Come inside . \n",
            "번역문 : 안으로 들어와 . \n",
            "예측문 :  노래 부탁해 . <eos>\n",
            "\n",
            "\n",
            "원문 :  I am homesick . \n",
            "번역문 : 나 향수병 걸렸어 . \n",
            "예측문 :  나 향수병 걸렸어 . <eos>\n",
            "\n",
            "\n",
            "원문 :  Nobody s perfect . \n",
            "번역문 : 완벽한 사람은 없어 . \n",
            "예측문 :  그 사람은 고양이를 키워 . <eos>\n",
            "\n",
            "\n",
            "원문 :  I did nothing wrong . \n",
            "번역문 : 난 잘못한 거 없어 . \n",
            "예측문 :  난 아주 고립된 기분이었어 . <eos>\n",
            "\n",
            "\n",
            "원문 :  Tom plays the saxophone . \n",
            "번역문 : 톰은 색소폰을 연주할 수 있어 . \n",
            "예측문 :  톰은 메리의 감정을 상하게 했다 . <eos>\n",
            "\n",
            "\n",
            "원문 :  Tom is shouting something . \n",
            "번역문 : 톰은 뭔가를 쏘고 있어 . \n",
            "예측문 :  톰은 운전할 수 있어 . <eos>\n",
            "\n",
            "\n",
            "원문 :  I m not sure what they want . \n",
            "번역문 : 그들이 원하는 게 뭔지 모르겠다 . \n",
            "예측문 :  난 톰이 자폐증이란 걸 알고 있어 . <eos>\n",
            "\n",
            "\n",
            "원문 :  She has a cat . The cat is white . \n",
            "번역문 : 그 사람은 고양이를 키워 . 그 고양이는 흰색 고양이야 . \n",
            "예측문 :  그는 나를 기다리게 했다 . <eos>\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSmAvOHbBvIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926219c8-7aa2-49c7-b289-1337f43b656b"
      },
      "source": [
        "# attention layer\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # values: hidden states from encoder\n",
        "        # query: hiden states from decoder, (batch_size, hidden_size)\n",
        "\n",
        "        ht = tf.expand_dims(query, 1) # (batch_size, 1, hidden_size)\n",
        "        score = self.V(tf.nn.tanh(self.W1(ht) + self.W2(values)))\n",
        "        alpha_t = tf.nn.softmax(score, axis = 1) #(batch_size, max_length, 1), attention_weights\n",
        "        context_vector = alpha_t * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
        "\n",
        "        return context_vector, alpha_t\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.models.Model):\n",
        "    def __init__(self, embed_dims):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed = tf.keras.layers.Embedding(src_vocab_size, embed_dims)\n",
        "        self.lstm = tf.keras.layers.LSTM(256, return_sequences = True, return_state = True)\n",
        "        self.masking = tf.keras.layers.Masking(mask_value = 0.0)\n",
        "\n",
        "    def call(self, inputs, h_init = None, c_init = None):\n",
        "        x = self.embed(inputs)\n",
        "        x = self.masking(x)\n",
        "        x, h, c = self.lstm(x, initial_state = [h_init, c_init])\n",
        "        return x, h, c\n",
        "\n",
        "class Decoder(tf.keras.models.Model):\n",
        "    def __init__(self, embed_dims):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed = tf.keras.layers.Embedding(tar_vocab_size, embed_dims)\n",
        "        self.lstm = tf.keras.layers.LSTM(256, return_state = True, return_sequences=True)\n",
        "        self.masking = tf.keras.layers.Masking(mask_value = 0.0)\n",
        "        self.attention = BahdanauAttention(embed_dims)\n",
        "        self.softmax = tf.keras.layers.Dense(tar_vocab_size, activation = \"softmax\")\n",
        "\n",
        "    def call(self, x, h, c, enc_output):\n",
        "        \n",
        "        # 최초에는 t-1번째 hidden_state(decoder) = enc_output\n",
        "        # x: decoder input\n",
        "        # h: encoder hidden state(last)\n",
        "        # c: encoder cell state(last)\n",
        "\n",
        "        # attention units: embed_dims\n",
        "        # context_vector: (batch_size, embed_dims)\n",
        "        # alpha_t : (batch_size, max_len)\n",
        "        context_vector, alpha_t = self.attention(h, enc_output)\n",
        "\n",
        "        # embedding: (batch_size, target_max_len, embed_dims)\n",
        "        x = self.embed(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1)\n",
        "\n",
        "        x, dec_h, dec_c = self.lstm(x,initial_state = [h,c])\n",
        "        outputs = self.softmax(x)\n",
        "\n",
        "        return outputs, dec_h, dec_c\n",
        "\n",
        "\n",
        "# dataloader with batch size\n",
        "\n",
        "def data_batch(train_x_enc, train_x_dec, train_y_dec, batch_size = 32):\n",
        "\n",
        "    # shuffle data\n",
        "    index = np.arange(0, train_x_enc.shape[0])\n",
        "    np.random.shuffle(index)\n",
        "\n",
        "    # batch\n",
        "    epochs = int(train_x_enc.shape[0] / batch_size)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        batch_x_enc = train_x_enc[batch_size * epoch : batch_size * (epoch + 1),:]\n",
        "        batch_x_dec = train_x_dec[batch_size * epoch : batch_size * (epoch + 1),:]\n",
        "\n",
        "        batch_y_dec = train_y_dec[batch_size * epoch : batch_size * (epoch + 1),:]\n",
        "\n",
        "        yield batch_x_enc, batch_x_dec, batch_y_dec\n",
        "\n",
        "\n",
        "# model train\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = \"none\")\n",
        "\n",
        "def loss_func(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss = loss_obj(real, pred)\n",
        "    mask = tf.cast(mask, dtype = loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "# model build\n",
        "embed_dims = 64\n",
        "encoder = Encoder(embed_dims)\n",
        "decoder = Decoder(embed_dims)\n",
        "\n",
        "# model save\n",
        "import os\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder = encoder, decoder=decoder)\n",
        "\n",
        "\n",
        "# batch 단위 학습을 위한 training function\n",
        "def train_step(enc_inputs, dec_inputs, dec_outputs, batch_size):\n",
        "\n",
        "    loss = 0\n",
        "    enc_h_init = tf.zeros((batch_size, 256))\n",
        "    enc_c_init = tf.zeros((batch_size, 256))\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_state_h, enc_state_c = encoder(enc_inputs, enc_h_init, enc_c_init)\n",
        "        dec_state_h = enc_state_h\n",
        "        dec_state_c = enc_state_c\n",
        "\n",
        "\n",
        "        for t,k in zip(range(0, dec_inputs.shape[1]),range(0, dec_outputs.shape[1])):\n",
        "\n",
        "            dec_input = tf.expand_dims(dec_inputs[:,t],1)\n",
        "            dec_output = tf.expand_dims(dec_outputs[:,k],1)\n",
        "\n",
        "            pred, dec_state_h, dec_state_c = decoder(dec_input, dec_state_h, dec_state_c, enc_output)\n",
        "            loss += loss_func(dec_output, pred)\n",
        "\n",
        "    batch_loss = (loss / int(dec_outputs.shape[1]))\n",
        "\n",
        "    # Backword Propagation and update the weights params\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss\n",
        "\n",
        "EPOCHS = 64\n",
        "batch_size = 64\n",
        "epochs_per_steps = int(len(encoder_input_train) / batch_size)\n",
        "\n",
        "# training\n",
        "import time\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for step in range(epochs_per_steps):\n",
        "\n",
        "        enc_x_batch = encoder_input_train[batch_size * step: batch_size * (step + 1), :]\n",
        "        dec_x_batch = decoder_input_train[batch_size * step: batch_size * (step + 1), :]\n",
        "        dec_y_batch = decoder_output_train[batch_size * step: batch_size * (step + 1), :]\n",
        "\n",
        "        batch_loss = train_step(enc_x_batch, dec_x_batch, dec_y_batch, batch_size)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if step % 16 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, step + 1, batch_loss.numpy()))\n",
        "\n",
        "    # 에포크가 2번 실행될때마다 모델 저장 (체크포인트)\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "        \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / epochs_per_steps))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4930: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 1 Loss 2.4884\n",
            "Epoch 1 Batch 17 Loss 2.9436\n",
            "Epoch 1 Batch 33 Loss 3.4837\n",
            "Epoch 1 Loss 3.2718\n",
            "Time taken for 1 epoch 8.312528371810913 sec\n",
            "\n",
            "Epoch 2 Batch 1 Loss 1.8788\n",
            "Epoch 2 Batch 17 Loss 2.6905\n",
            "Epoch 2 Batch 33 Loss 3.1558\n",
            "Epoch 2 Loss 2.7672\n",
            "Time taken for 1 epoch 8.080798149108887 sec\n",
            "\n",
            "Epoch 3 Batch 1 Loss 1.9225\n",
            "Epoch 3 Batch 17 Loss 2.6976\n",
            "Epoch 3 Batch 33 Loss 3.0329\n",
            "Epoch 3 Loss 2.6921\n",
            "Time taken for 1 epoch 8.085687637329102 sec\n",
            "\n",
            "Epoch 4 Batch 1 Loss 1.9951\n",
            "Epoch 4 Batch 17 Loss 2.6172\n",
            "Epoch 4 Batch 33 Loss 2.9563\n",
            "Epoch 4 Loss 2.6144\n",
            "Time taken for 1 epoch 8.07849931716919 sec\n",
            "\n",
            "Epoch 5 Batch 1 Loss 1.6350\n",
            "Epoch 5 Batch 17 Loss 2.5296\n",
            "Epoch 5 Batch 33 Loss 2.9033\n",
            "Epoch 5 Loss 2.5198\n",
            "Time taken for 1 epoch 8.11619758605957 sec\n",
            "\n",
            "Epoch 6 Batch 1 Loss 1.4330\n",
            "Epoch 6 Batch 17 Loss 2.4492\n",
            "Epoch 6 Batch 33 Loss 2.8124\n",
            "Epoch 6 Loss 2.4283\n",
            "Time taken for 1 epoch 8.09291696548462 sec\n",
            "\n",
            "Epoch 7 Batch 1 Loss 1.2993\n",
            "Epoch 7 Batch 17 Loss 2.3909\n",
            "Epoch 7 Batch 33 Loss 2.7660\n",
            "Epoch 7 Loss 2.3651\n",
            "Time taken for 1 epoch 8.004329442977905 sec\n",
            "\n",
            "Epoch 8 Batch 1 Loss 1.2293\n",
            "Epoch 8 Batch 17 Loss 2.3427\n",
            "Epoch 8 Batch 33 Loss 2.6991\n",
            "Epoch 8 Loss 2.3122\n",
            "Time taken for 1 epoch 8.014022588729858 sec\n",
            "\n",
            "Epoch 9 Batch 1 Loss 1.1699\n",
            "Epoch 9 Batch 17 Loss 2.2756\n",
            "Epoch 9 Batch 33 Loss 2.6560\n",
            "Epoch 9 Loss 2.2634\n",
            "Time taken for 1 epoch 8.061357498168945 sec\n",
            "\n",
            "Epoch 10 Batch 1 Loss 1.1472\n",
            "Epoch 10 Batch 17 Loss 2.2373\n",
            "Epoch 10 Batch 33 Loss 2.6137\n",
            "Epoch 10 Loss 2.2238\n",
            "Time taken for 1 epoch 8.265044689178467 sec\n",
            "\n",
            "Epoch 11 Batch 1 Loss 1.1050\n",
            "Epoch 11 Batch 17 Loss 2.2146\n",
            "Epoch 11 Batch 33 Loss 2.5764\n",
            "Epoch 11 Loss 2.1970\n",
            "Time taken for 1 epoch 8.027673244476318 sec\n",
            "\n",
            "Epoch 12 Batch 1 Loss 1.0378\n",
            "Epoch 12 Batch 17 Loss 2.1958\n",
            "Epoch 12 Batch 33 Loss 2.5535\n",
            "Epoch 12 Loss 2.1647\n",
            "Time taken for 1 epoch 8.073988437652588 sec\n",
            "\n",
            "Epoch 13 Batch 1 Loss 1.0094\n",
            "Epoch 13 Batch 17 Loss 2.1433\n",
            "Epoch 13 Batch 33 Loss 2.5017\n",
            "Epoch 13 Loss 2.1198\n",
            "Time taken for 1 epoch 7.960252046585083 sec\n",
            "\n",
            "Epoch 14 Batch 1 Loss 0.9833\n",
            "Epoch 14 Batch 17 Loss 2.1011\n",
            "Epoch 14 Batch 33 Loss 2.4578\n",
            "Epoch 14 Loss 2.0815\n",
            "Time taken for 1 epoch 8.114061117172241 sec\n",
            "\n",
            "Epoch 15 Batch 1 Loss 0.9533\n",
            "Epoch 15 Batch 17 Loss 2.0649\n",
            "Epoch 15 Batch 33 Loss 2.4144\n",
            "Epoch 15 Loss 2.0467\n",
            "Time taken for 1 epoch 7.984316110610962 sec\n",
            "\n",
            "Epoch 16 Batch 1 Loss 0.9330\n",
            "Epoch 16 Batch 17 Loss 2.0321\n",
            "Epoch 16 Batch 33 Loss 2.3723\n",
            "Epoch 16 Loss 2.0140\n",
            "Time taken for 1 epoch 8.108518600463867 sec\n",
            "\n",
            "Epoch 17 Batch 1 Loss 0.9084\n",
            "Epoch 17 Batch 17 Loss 1.9987\n",
            "Epoch 17 Batch 33 Loss 2.3251\n",
            "Epoch 17 Loss 1.9806\n",
            "Time taken for 1 epoch 7.962095499038696 sec\n",
            "\n",
            "Epoch 18 Batch 1 Loss 0.8987\n",
            "Epoch 18 Batch 17 Loss 1.9658\n",
            "Epoch 18 Batch 33 Loss 2.2841\n",
            "Epoch 18 Loss 1.9490\n",
            "Time taken for 1 epoch 8.040393352508545 sec\n",
            "\n",
            "Epoch 19 Batch 1 Loss 0.8813\n",
            "Epoch 19 Batch 17 Loss 1.9385\n",
            "Epoch 19 Batch 33 Loss 2.2469\n",
            "Epoch 19 Loss 1.9198\n",
            "Time taken for 1 epoch 7.985455751419067 sec\n",
            "\n",
            "Epoch 20 Batch 1 Loss 0.8760\n",
            "Epoch 20 Batch 17 Loss 1.9255\n",
            "Epoch 20 Batch 33 Loss 2.2085\n",
            "Epoch 20 Loss 1.8948\n",
            "Time taken for 1 epoch 8.011935472488403 sec\n",
            "\n",
            "Epoch 21 Batch 1 Loss 0.8599\n",
            "Epoch 21 Batch 17 Loss 1.8813\n",
            "Epoch 21 Batch 33 Loss 2.1808\n",
            "Epoch 21 Loss 1.8692\n",
            "Time taken for 1 epoch 8.22406005859375 sec\n",
            "\n",
            "Epoch 22 Batch 1 Loss 0.8331\n",
            "Epoch 22 Batch 17 Loss 1.8542\n",
            "Epoch 22 Batch 33 Loss 2.1286\n",
            "Epoch 22 Loss 1.8370\n",
            "Time taken for 1 epoch 8.08142638206482 sec\n",
            "\n",
            "Epoch 23 Batch 1 Loss 0.8098\n",
            "Epoch 23 Batch 17 Loss 1.8244\n",
            "Epoch 23 Batch 33 Loss 2.0950\n",
            "Epoch 23 Loss 1.8098\n",
            "Time taken for 1 epoch 7.9378979206085205 sec\n",
            "\n",
            "Epoch 24 Batch 1 Loss 0.7830\n",
            "Epoch 24 Batch 17 Loss 1.8002\n",
            "Epoch 24 Batch 33 Loss 2.0579\n",
            "Epoch 24 Loss 1.7809\n",
            "Time taken for 1 epoch 8.041647672653198 sec\n",
            "\n",
            "Epoch 25 Batch 1 Loss 0.7686\n",
            "Epoch 25 Batch 17 Loss 1.7905\n",
            "Epoch 25 Batch 33 Loss 2.0250\n",
            "Epoch 25 Loss 1.7547\n",
            "Time taken for 1 epoch 8.01282024383545 sec\n",
            "\n",
            "Epoch 26 Batch 1 Loss 0.7922\n",
            "Epoch 26 Batch 17 Loss 1.7349\n",
            "Epoch 26 Batch 33 Loss 2.0257\n",
            "Epoch 26 Loss 1.7305\n",
            "Time taken for 1 epoch 7.895717620849609 sec\n",
            "\n",
            "Epoch 27 Batch 1 Loss 0.7546\n",
            "Epoch 27 Batch 17 Loss 1.6927\n",
            "Epoch 27 Batch 33 Loss 1.9553\n",
            "Epoch 27 Loss 1.6859\n",
            "Time taken for 1 epoch 8.02010464668274 sec\n",
            "\n",
            "Epoch 28 Batch 1 Loss 0.7399\n",
            "Epoch 28 Batch 17 Loss 1.6522\n",
            "Epoch 28 Batch 33 Loss 1.9058\n",
            "Epoch 28 Loss 1.6537\n",
            "Time taken for 1 epoch 8.030340909957886 sec\n",
            "\n",
            "Epoch 29 Batch 1 Loss 0.7809\n",
            "Epoch 29 Batch 17 Loss 1.6281\n",
            "Epoch 29 Batch 33 Loss 1.8853\n",
            "Epoch 29 Loss 1.6231\n",
            "Time taken for 1 epoch 8.032631397247314 sec\n",
            "\n",
            "Epoch 30 Batch 1 Loss 0.7616\n",
            "Epoch 30 Batch 17 Loss 1.5855\n",
            "Epoch 30 Batch 33 Loss 1.8215\n",
            "Epoch 30 Loss 1.5952\n",
            "Time taken for 1 epoch 8.138791561126709 sec\n",
            "\n",
            "Epoch 31 Batch 1 Loss 0.7193\n",
            "Epoch 31 Batch 17 Loss 1.5839\n",
            "Epoch 31 Batch 33 Loss 1.7706\n",
            "Epoch 31 Loss 1.5647\n",
            "Time taken for 1 epoch 8.284149885177612 sec\n",
            "\n",
            "Epoch 32 Batch 1 Loss 0.6983\n",
            "Epoch 32 Batch 17 Loss 1.5280\n",
            "Epoch 32 Batch 33 Loss 1.7178\n",
            "Epoch 32 Loss 1.5266\n",
            "Time taken for 1 epoch 8.09116244316101 sec\n",
            "\n",
            "Epoch 33 Batch 1 Loss 0.6887\n",
            "Epoch 33 Batch 17 Loss 1.4918\n",
            "Epoch 33 Batch 33 Loss 1.6786\n",
            "Epoch 33 Loss 1.4958\n",
            "Time taken for 1 epoch 8.080763578414917 sec\n",
            "\n",
            "Epoch 34 Batch 1 Loss 0.6796\n",
            "Epoch 34 Batch 17 Loss 1.4790\n",
            "Epoch 34 Batch 33 Loss 1.6446\n",
            "Epoch 34 Loss 1.4666\n",
            "Time taken for 1 epoch 7.978981971740723 sec\n",
            "\n",
            "Epoch 35 Batch 1 Loss 0.7025\n",
            "Epoch 35 Batch 17 Loss 1.4316\n",
            "Epoch 35 Batch 33 Loss 1.6034\n",
            "Epoch 35 Loss 1.4480\n",
            "Time taken for 1 epoch 7.987682580947876 sec\n",
            "\n",
            "Epoch 36 Batch 1 Loss 0.7054\n",
            "Epoch 36 Batch 17 Loss 1.4705\n",
            "Epoch 36 Batch 33 Loss 1.5660\n",
            "Epoch 36 Loss 1.4458\n",
            "Time taken for 1 epoch 8.072681188583374 sec\n",
            "\n",
            "Epoch 37 Batch 1 Loss 0.6821\n",
            "Epoch 37 Batch 17 Loss 1.4491\n",
            "Epoch 37 Batch 33 Loss 1.5242\n",
            "Epoch 37 Loss 1.4122\n",
            "Time taken for 1 epoch 7.984440326690674 sec\n",
            "\n",
            "Epoch 38 Batch 1 Loss 0.6726\n",
            "Epoch 38 Batch 17 Loss 1.3953\n",
            "Epoch 38 Batch 33 Loss 1.4877\n",
            "Epoch 38 Loss 1.3823\n",
            "Time taken for 1 epoch 8.10652470588684 sec\n",
            "\n",
            "Epoch 39 Batch 1 Loss 0.6452\n",
            "Epoch 39 Batch 17 Loss 1.3762\n",
            "Epoch 39 Batch 33 Loss 1.4556\n",
            "Epoch 39 Loss 1.3506\n",
            "Time taken for 1 epoch 7.9955151081085205 sec\n",
            "\n",
            "Epoch 40 Batch 1 Loss 0.6329\n",
            "Epoch 40 Batch 17 Loss 1.3535\n",
            "Epoch 40 Batch 33 Loss 1.4112\n",
            "Epoch 40 Loss 1.3211\n",
            "Time taken for 1 epoch 8.07236933708191 sec\n",
            "\n",
            "Epoch 41 Batch 1 Loss 0.6662\n",
            "Epoch 41 Batch 17 Loss 1.3087\n",
            "Epoch 41 Batch 33 Loss 1.3788\n",
            "Epoch 41 Loss 1.2922\n",
            "Time taken for 1 epoch 8.04689359664917 sec\n",
            "\n",
            "Epoch 42 Batch 1 Loss 0.6714\n",
            "Epoch 42 Batch 17 Loss 1.2712\n",
            "Epoch 42 Batch 33 Loss 1.3789\n",
            "Epoch 42 Loss 1.2708\n",
            "Time taken for 1 epoch 8.234936714172363 sec\n",
            "\n",
            "Epoch 43 Batch 1 Loss 0.6838\n",
            "Epoch 43 Batch 17 Loss 1.2732\n",
            "Epoch 43 Batch 33 Loss 1.3397\n",
            "Epoch 43 Loss 1.2446\n",
            "Time taken for 1 epoch 8.023057222366333 sec\n",
            "\n",
            "Epoch 44 Batch 1 Loss 0.6376\n",
            "Epoch 44 Batch 17 Loss 1.2354\n",
            "Epoch 44 Batch 33 Loss 1.2875\n",
            "Epoch 44 Loss 1.2144\n",
            "Time taken for 1 epoch 8.11788034439087 sec\n",
            "\n",
            "Epoch 45 Batch 1 Loss 0.6417\n",
            "Epoch 45 Batch 17 Loss 1.1978\n",
            "Epoch 45 Batch 33 Loss 1.2438\n",
            "Epoch 45 Loss 1.1874\n",
            "Time taken for 1 epoch 7.991312265396118 sec\n",
            "\n",
            "Epoch 46 Batch 1 Loss 0.6276\n",
            "Epoch 46 Batch 17 Loss 1.1735\n",
            "Epoch 46 Batch 33 Loss 1.2059\n",
            "Epoch 46 Loss 1.1641\n",
            "Time taken for 1 epoch 8.124302625656128 sec\n",
            "\n",
            "Epoch 47 Batch 1 Loss 0.6011\n",
            "Epoch 47 Batch 17 Loss 1.1569\n",
            "Epoch 47 Batch 33 Loss 1.1811\n",
            "Epoch 47 Loss 1.1330\n",
            "Time taken for 1 epoch 8.035249948501587 sec\n",
            "\n",
            "Epoch 48 Batch 1 Loss 0.5976\n",
            "Epoch 48 Batch 17 Loss 1.1415\n",
            "Epoch 48 Batch 33 Loss 1.1676\n",
            "Epoch 48 Loss 1.1076\n",
            "Time taken for 1 epoch 8.197861433029175 sec\n",
            "\n",
            "Epoch 49 Batch 1 Loss 0.5843\n",
            "Epoch 49 Batch 17 Loss 1.1028\n",
            "Epoch 49 Batch 33 Loss 1.1279\n",
            "Epoch 49 Loss 1.0868\n",
            "Time taken for 1 epoch 8.071750402450562 sec\n",
            "\n",
            "Epoch 50 Batch 1 Loss 0.5718\n",
            "Epoch 50 Batch 17 Loss 1.0757\n",
            "Epoch 50 Batch 33 Loss 1.0792\n",
            "Epoch 50 Loss 1.0595\n",
            "Time taken for 1 epoch 8.037608861923218 sec\n",
            "\n",
            "Epoch 51 Batch 1 Loss 0.5694\n",
            "Epoch 51 Batch 17 Loss 1.0448\n",
            "Epoch 51 Batch 33 Loss 1.0363\n",
            "Epoch 51 Loss 1.0319\n",
            "Time taken for 1 epoch 7.988483190536499 sec\n",
            "\n",
            "Epoch 52 Batch 1 Loss 0.5684\n",
            "Epoch 52 Batch 17 Loss 1.0510\n",
            "Epoch 52 Batch 33 Loss 1.0372\n",
            "Epoch 52 Loss 1.0196\n",
            "Time taken for 1 epoch 8.04273271560669 sec\n",
            "\n",
            "Epoch 53 Batch 1 Loss 0.5601\n",
            "Epoch 53 Batch 17 Loss 1.0367\n",
            "Epoch 53 Batch 33 Loss 0.9982\n",
            "Epoch 53 Loss 1.0019\n",
            "Time taken for 1 epoch 8.212286233901978 sec\n",
            "\n",
            "Epoch 54 Batch 1 Loss 0.5590\n",
            "Epoch 54 Batch 17 Loss 0.9678\n",
            "Epoch 54 Batch 33 Loss 0.9537\n",
            "Epoch 54 Loss 0.9717\n",
            "Time taken for 1 epoch 8.100855350494385 sec\n",
            "\n",
            "Epoch 55 Batch 1 Loss 0.5549\n",
            "Epoch 55 Batch 17 Loss 0.9639\n",
            "Epoch 55 Batch 33 Loss 0.9361\n",
            "Epoch 55 Loss 0.9453\n",
            "Time taken for 1 epoch 8.025762796401978 sec\n",
            "\n",
            "Epoch 56 Batch 1 Loss 0.5663\n",
            "Epoch 56 Batch 17 Loss 0.9229\n",
            "Epoch 56 Batch 33 Loss 0.8990\n",
            "Epoch 56 Loss 0.9235\n",
            "Time taken for 1 epoch 8.06136679649353 sec\n",
            "\n",
            "Epoch 57 Batch 1 Loss 0.5418\n",
            "Epoch 57 Batch 17 Loss 0.9178\n",
            "Epoch 57 Batch 33 Loss 0.8878\n",
            "Epoch 57 Loss 0.9099\n",
            "Time taken for 1 epoch 8.06681227684021 sec\n",
            "\n",
            "Epoch 58 Batch 1 Loss 0.5233\n",
            "Epoch 58 Batch 17 Loss 0.8998\n",
            "Epoch 58 Batch 33 Loss 0.8858\n",
            "Epoch 58 Loss 0.8969\n",
            "Time taken for 1 epoch 7.908901929855347 sec\n",
            "\n",
            "Epoch 59 Batch 1 Loss 0.5313\n",
            "Epoch 59 Batch 17 Loss 0.8931\n",
            "Epoch 59 Batch 33 Loss 0.9458\n",
            "Epoch 59 Loss 0.8904\n",
            "Time taken for 1 epoch 8.079243659973145 sec\n",
            "\n",
            "Epoch 60 Batch 1 Loss 0.5413\n",
            "Epoch 60 Batch 17 Loss 0.8941\n",
            "Epoch 60 Batch 33 Loss 0.8625\n",
            "Epoch 60 Loss 0.8761\n",
            "Time taken for 1 epoch 8.06368350982666 sec\n",
            "\n",
            "Epoch 61 Batch 1 Loss 0.5502\n",
            "Epoch 61 Batch 17 Loss 0.8537\n",
            "Epoch 61 Batch 33 Loss 0.8260\n",
            "Epoch 61 Loss 0.8571\n",
            "Time taken for 1 epoch 8.065968751907349 sec\n",
            "\n",
            "Epoch 62 Batch 1 Loss 0.5249\n",
            "Epoch 62 Batch 17 Loss 0.8124\n",
            "Epoch 62 Batch 33 Loss 0.7980\n",
            "Epoch 62 Loss 0.8235\n",
            "Time taken for 1 epoch 8.06383466720581 sec\n",
            "\n",
            "Epoch 63 Batch 1 Loss 0.5204\n",
            "Epoch 63 Batch 17 Loss 0.7833\n",
            "Epoch 63 Batch 33 Loss 0.7573\n",
            "Epoch 63 Loss 0.7967\n",
            "Time taken for 1 epoch 8.293991327285767 sec\n",
            "\n",
            "Epoch 64 Batch 1 Loss 0.5251\n",
            "Epoch 64 Batch 17 Loss 0.7523\n",
            "Epoch 64 Batch 33 Loss 0.7400\n",
            "Epoch 64 Loss 0.7785\n",
            "Time taken for 1 epoch 8.057565689086914 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW6T4hqu7nwm",
        "outputId": "a88edbff-3b13-40f1-a563-e9d8ef2c353b"
      },
      "source": [
        "# prediction(translate)\n",
        "\n",
        "def decode_sequence(input_seq, encoder_model, decoder_model):\n",
        "\n",
        "    \n",
        "    enc_h_init = tf.zeros((1, 256))\n",
        "    enc_c_init = tf.zeros((1, 256))\n",
        "    \n",
        "\n",
        "    # encoder\n",
        "    enc_output, enc_state_h, enc_state_c = encoder(input_seq, enc_h_init, enc_c_init)\n",
        "\n",
        "    # decoder\n",
        "    dec_state_h = enc_state_h\n",
        "    dec_state_c = enc_state_c\n",
        "\n",
        "    #<sos>에 대응되는 정수 인덱스 부여\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0,0] = tar_to_index['<sos>']\n",
        "    \n",
        "    stop_cond = False\n",
        "    decode_sentence = ''\n",
        "\n",
        "    while not stop_cond:\n",
        "\n",
        "        output_tokens, dec_state_h, dec_state_c = decoder_model(target_seq, dec_state_h, dec_state_c, enc_output)\n",
        "\n",
        "        # 예측 결과를 단어로 변환\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        if sampled_token_index ==0:\n",
        "            break\n",
        "        else:\n",
        "            sampled_char = index_to_tar[sampled_token_index]\n",
        "\n",
        "        # 결과를 문장에 추가\n",
        "        decode_sentence += \" \" + sampled_char \n",
        "\n",
        "        # stop condition\n",
        "        if (sampled_char == '<eos>' or len(decode_sentence) >= 50):\n",
        "            stop_cond = True\n",
        "\n",
        "        # 예측된 결과를 다음 hidden_state로 반영\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0,0] = sampled_token_index\n",
        "    \n",
        "    return decode_sentence\n",
        "\n",
        "\n",
        "def seq2src(input_seq):\n",
        "    temp=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            temp = temp + index_to_src[i]+' '\n",
        "    return temp\n",
        "\n",
        "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2tar(input_seq):\n",
        "    temp=''\n",
        "    for i in input_seq:\n",
        "        if((i!= 0 and i!=tar_to_index['<sos>']) and i!=tar_to_index['<eos>']):\n",
        "            temp = temp + index_to_tar[i] + ' '\n",
        "    return temp\n",
        "\n",
        "for seq_index in [3,10,25,50,100,150,300,500,750,1000,1500, 1750, 2000, 2500]:\n",
        "    input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq, encoder, decoder)\n",
        "\n",
        "    print(\"원문 : \",seq2src(encoder_input_train[seq_index]))\n",
        "    print(\"번역문 :\",seq2tar(decoder_input_train[seq_index]))\n",
        "    print(\"예측문 :\",decoded_sentence)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "원문 :  Who ? \n",
            "번역문 : 누구 ? \n",
            "예측문 :  누구 집에 있어 ? <eos>\n",
            "\n",
            "\n",
            "원문 :  Wait ! \n",
            "번역문 : 잠깐 ! \n",
            "예측문 :  이렇게 매력적일 수가 ! <eos>\n",
            "\n",
            "\n",
            "원문 :  Got it ! \n",
            "번역문 : 알겠어 ! \n",
            "예측문 :  이렇게 매력적일 수가 ! <eos>\n",
            "\n",
            "\n",
            "원문 :  Hit Tom . \n",
            "번역문 : 톰을 때려 . \n",
            "예측문 :  톰이 외로워하는지 궁금해 . <eos>\n",
            "\n",
            "\n",
            "원문 :  Tom paid . \n",
            "번역문 : 톰이 지불했어 . \n",
            "예측문 :  톰이 사기 쳤어 . <eos>\n",
            "\n",
            "\n",
            "원문 :  Of course . \n",
            "번역문 : 물론이죠 . \n",
            "예측문 :  빨리 마 . <eos>\n",
            "\n",
            "\n",
            "원문 :  Come inside . \n",
            "번역문 : 안으로 들어와 . \n",
            "예측문 :  그만 이것좀 봐 . <eos>\n",
            "\n",
            "\n",
            "원문 :  I am homesick . \n",
            "번역문 : 나 향수병 걸렸어 . \n",
            "예측문 :  나는 이유 없이 슬펐어 . <eos>\n",
            "\n",
            "\n",
            "원문 :  Nobody s perfect . \n",
            "번역문 : 완벽한 사람은 없어 . \n",
            "예측문 :  아무도 날 알아보지 못 했어 . <eos>\n",
            "\n",
            "\n",
            "원문 :  I did nothing wrong . \n",
            "번역문 : 난 잘못한 거 없어 . \n",
            "예측문 :  난 널 도와주려고 노력하던 중이었어 . <eos>\n",
            "\n",
            "\n",
            "원문 :  Tom plays the saxophone . \n",
            "번역문 : 톰은 색소폰을 연주할 수 있어 . \n",
            "예측문 :  톰은 나이에 비해 키가 매우 성숙해 . <eos>\n",
            "\n",
            "\n",
            "원문 :  Tom is shouting something . \n",
            "번역문 : 톰은 뭔가를 쏘고 있어 . \n",
            "예측문 :  톰은 나이에 비해 키가 매우 성숙해 . <eos>\n",
            "\n",
            "\n",
            "원문 :  I m not sure what they want . \n",
            "번역문 : 그들이 원하는 게 뭔지 모르겠다 . \n",
            "예측문 :  난 톰이 웃는 걸 좋아해 . <eos>\n",
            "\n",
            "\n",
            "원문 :  She has a cat . The cat is white . \n",
            "번역문 : 그 사람은 고양이를 키워 . 그 고양이는 흰색 고양이야 . \n",
            "예측문 :  그 사람은 방 주위을 둘러 봤어 . <eos>\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}